\documentclass[11pt]{article}

%% FONTS
%% To get the default sans serif font in latex, uncomment following line:
 \renewcommand*\familydefault{\sfdefault}
%%
%% to get Arial font as the sans serif font, uncomment following line:
%% \renewcommand{\sfdefault}{phv} % phv is the Arial font
%%
%% to get Helvetica font as the sans serif font, uncomment following line:
% \usepackage{helvet}
\usepackage[small,bf,up]{caption}
\renewcommand{\captionfont}{\footnotesize}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}
\usepackage{graphics,epsfig,graphicx,float,subfigure,color}
\usepackage{amsmath,amssymb,amsbsy,amsfonts,amsthm}
\usepackage{url}
\usepackage{boxedminipage}
\usepackage[sf,bf,tiny]{titlesec}
 \usepackage[plainpages=false, colorlinks=true,
   citecolor=blue, filecolor=blue, linkcolor=blue,
   urlcolor=blue]{hyperref}
\usepackage{enumitem}

\newcommand{\todo}[1]{\textcolor{red}{#1}}
% see documentation for titlesec package
% \titleformat{\section}{\large \sffamily \bfseries}
\titlelabel{\thetitle.\,\,\,}

\newcommand{\bs}{\boldsymbol}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\setlength{\emergencystretch}{20pt}

\begin{document}


\begin{center}
  \vspace*{-2cm}
{\small MATH-GA 2012.001 and CSCI-GA 2945.001, Georg Stadler \&
  Dhairya Malhotra (NYU Courant)}
\end{center}
\vspace*{.5cm}
\begin{center}
\large \textbf{%%
High Performance Computing \\
Anthony Trubiano \\
Assignment 5 (due April 29, 2019) }
\end{center}

% ****************************

\noindent {\bf Note:} All CPU computations were performed with an Intel Core i7-8750H Processor with base frequency 2.20 GHz. The maximum main memory bandwidth is 41.8 GB/s. At 16 double 
precision operations per cycle, the theoretical max flop rate would be about 35.2 GFlops/s. It has $6$ cores and can reach $12$ threads through hyper-threading. 

\begin{enumerate}
% --------------------------

\item {\bf MPI Ring Communication} Ring communication was implemented using MPI. The 0th process is given the value $0$, which then gets passed cyclically to each process and gets that process's rank added to it. This is repeated $N$ times, and the result is compared to the expected solution, $NM(M-1)/2$, where $M$ is the number of processes. We use this to estimate the latency of message passing, and then pass a $2$ MB array to test bandwidth. We use $N=10000$ and get the following results:

\begin{table}[h!] 
	\centering
	\begin{tabular}{c | c c}
		Processes & Latency (ms) & Bandwidth (GB/s)  \\
		\hline
		2 & 0.00057 & 2.32 \\
		3 & 0.00178 & 1.2 \\ 
		4 & 0.0018 & 0.75 \\
		5 & 0.002 & 0.55 \\
	\end{tabular}
	\caption{Latency and Bandwidth for message passing using MPI as a function of number of processes. }
\end{table}

From here we see that as we increase the number of processes, the latency increases and the bandwidth decreases. This is to be expected as it takes longer to communicate when processes are not on the same core. 

I attempted to run MPI over a network using the different Crunchy servers, but the code kept hanging. I am not sure why.


\item {\bf Project}

\begin{center}
	\begin{tabular} {|c|p{9cm}|p{2cm}|}
		\hline
		\multicolumn{3}{|c|}{\bf Project: A parallel implementation of the IB method} \\
		\hline
		Week & Work & Who  \\ \hline \hline
		04/15-04/21 & Think about
		algorithm. How to do spread in parallel? OpenMP or MPI? Which FFT library to use? &  AT, TG, OM \\ \hline
		04/22-04/28 & Write OpenMP version of spread and interpolate & OM \\ \hline
		04/22-04/28 & Research OpenMP, MPI FFTW algorithms & TG, AT \\ \hline
		04/29-05/05 & Write parallel fluid solver & TG, AT \\ \hline
		04/29-05/05 & Write C++ initialization routines, force calculation & OM \\ \hline
		05/06-05/12 & Finish implementation, think about possible GPU or MPI implementation of spread and interpolate & AT, TG, OM \\ \hline
		05/13-05/19 & Run scalability tests, work on
		presentation slides and report  & TG, AT, OM \\ \hline
	\end{tabular}
\end{center} 














\end{enumerate}

\end{document}
